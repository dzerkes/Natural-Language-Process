{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ΜΑΡΙΑ ΚΑΙΚΤΖΟΓΛΟΥ - 03400052\n",
    "# ΔΗΜΗΤΡΗΣ ΖΕΡΚΕΛΙΔΗΣ - 03400049"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ΜΕΡΟΣ 2\n",
    "## Χρήση σημασιολογικών αναπαραστάσεων για ανάλυση συναισθήματος"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 16: Δεδομένα και προεπεξεργασία"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(a)__ Κατεβάζουμε τα δεδομένα που μας ζητάει στο ερώτημα από το \n",
    "\n",
    "http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(b)__ Προεπεξεργασία δεδομένων\n",
    "\n",
    "Χρησιμοποιούμε τον κώδικα από το github, για το δίαβασμα των δεδομένων και κάνουμε μια αλλαγή στη συνάρτηση tokenize, ώστε να διαβάζει μια λίστα απο προτάσεις και όχι μια λίστα ξεχωριστά για να μας επιστρέφει tokenized sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_dir = './aclImdb/'\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "test_dir = os.path.join(data_dir, 'test')\n",
    "pos_train_dir = os.path.join(train_dir, 'pos')\n",
    "neg_train_dir = os.path.join(train_dir, 'neg')\n",
    "pos_test_dir = os.path.join(test_dir, 'pos')\n",
    "neg_test_dir = os.path.join(test_dir, 'neg')\n",
    "\n",
    "# For memory limitations. These parameters fit in 8GB of RAM.\n",
    "# If you have 16G of RAM you can experiment with the full dataset / W2V\n",
    "\n",
    "MAX_NUM_SAMPLES = 5000 \n",
    "# Load first 1M word embeddings. This works because GoogleNews are roughly\n",
    "# sorted from most frequent to least frequent.\n",
    "# It may yield much worse results for other embeddings corpora\n",
    "NUM_W2V_TO_LOAD = 100000 # το μείωσαμε λόγω ram\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "# Fix numpy random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "try:\n",
    "    import glob2 as glob\n",
    "except ImportError:\n",
    "    import glob\n",
    "\n",
    "import re\n",
    "\n",
    "def strip_punctuation(s):\n",
    "    return re.sub(r'[^a-zA-Z\\s]', ' ', s)\n",
    "\n",
    "def preprocess(s):\n",
    "    return re.sub('\\s+',' ', strip_punctuation(s).lower())\n",
    "\n",
    "def tokenize(s):\n",
    "    array =[]\n",
    "    for i in s:\n",
    "        tmp = i.split(' ')\n",
    "        tmp_arr = [x for x in tmp]\n",
    "        array.append(tmp_arr)\n",
    "    return array\n",
    "\n",
    "def preproc_tok(s):\n",
    "    return tokenize(preprocess(s))\n",
    "\n",
    "def read_samples(folder, preprocess=lambda x: x):\n",
    "    samples = glob.iglob(os.path.join(folder, '*.txt'))\n",
    "    data = []\n",
    "    for i, sample in enumerate(samples):\n",
    "        if MAX_NUM_SAMPLES > 0 and i == MAX_NUM_SAMPLES:\n",
    "            break\n",
    "        with open(sample, 'r') as fd:\n",
    "            x = [preprocess(l) for l in fd][0]\n",
    "            data.append(x)\n",
    "    return data\n",
    "\n",
    "def create_corpus(pos, neg):\n",
    "    corpus = np.array(pos + neg) # enwnw tis 2 listes \n",
    "    y = np.array([1 for _ in pos] + [0 for _ in neg]) # ftiaxnw lista y me 1 sta pos kai 0 sta neg\n",
    "    indices = np.arange(y.shape[0]) # anakatevw ta dedomena mou gia kalitero training\n",
    "    np.random.shuffle(indices)\n",
    "    return list(corpus[indices]), list(y[indices]) # epistrefw lista me 2 ipolistes , mia gia to review,mia gia to label 1 i 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_train = read_samples(pos_train_dir,preprocess )\n",
    "positive_test = read_samples(pos_test_dir,preprocess )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_train = read_samples(neg_train_dir,preprocess )\n",
    "negative_test  = read_samples(neg_test_dir,preprocess )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Παρακάτω βλέπουμε τη δομή των παραπάνω λιστών που φτιάξαμε. Είναι όπως φαίνεται το κάθε entry της λίστας ένα string που αντιπροσοπεύει τα reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i was lucky enough to catch this movie while volunteering at the maryland film festival i ve always been a fan of classic horror films and especially the gimmicks of william castle so this was definitely a must see for me br br this is about the life and work of william castle who in my opinion was an underrated director true he made some cheap budget schlocky horror films but he added something to these films real live theater gimmicks that you don t see anymore for example he had nurses in case someone had a heart attack at his movies and put vibrators at the bottoms of chairs in the tingler br br this is truly a well made documentary and brings this rather shadowed director into the light and celebrated his contributions to horror cinema it also paints castle as a larger than life character who was very well liked and had a smile on his face br br unlike most film documentaries that mostly show testaments from film historians spine tingler shows interviews mostly from his family members and directors who were influenced by his work such as john waters john landis and joe dante a must see for classic horror and sci fi fans ',\n",
       " 'i was years old when this movie premiered on the television being raised in texas i understood the boredom monotony of teenage life there this movie touched my impressionable teenage heart i remembered it fondly through the past years i recently got to see it for the nd rd th times thanks the the love channel i still cry because the movie reaches in touches my inner confused teenager ']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_train[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Αλλάξαμε λίγο την tokenize , ώστε να επεξεργάζεται λίστα με reviews και επιστρέφει μια tokenized λίστα ανά πρόταση.\n",
    "\n",
    "Δηλαδή έχουμε μια λίστα με υπολίστες tokenized sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_pos_train = tokenize(positive_train)\n",
    "tokenized_pos_test  = tokenize(positive_test)\n",
    "tokenized_neg_train = tokenize(negative_train)\n",
    "tokenized_neg_test  = tokenize(negative_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Παρακάτω θα δείξουμε ένα παράδειγμα μιας tokenized πρότασης."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['',\n",
       "  'out',\n",
       "  'of',\n",
       "  'br',\n",
       "  'br',\n",
       "  'this',\n",
       "  'film',\n",
       "  'was',\n",
       "  'neither',\n",
       "  'funny',\n",
       "  'as',\n",
       "  'a',\n",
       "  'whole',\n",
       "  'nor',\n",
       "  'was',\n",
       "  'it',\n",
       "  'even',\n",
       "  'worthing',\n",
       "  'investing',\n",
       "  'any',\n",
       "  'kind',\n",
       "  'of',\n",
       "  'emotion',\n",
       "  'into',\n",
       "  'the',\n",
       "  'characters',\n",
       "  'eugene',\n",
       "  'levy',\n",
       "  'is',\n",
       "  'probably',\n",
       "  'the',\n",
       "  'most',\n",
       "  'funny',\n",
       "  'the',\n",
       "  'rest',\n",
       "  'of',\n",
       "  'the',\n",
       "  'cast',\n",
       "  'do',\n",
       "  'their',\n",
       "  'jobs',\n",
       "  'but',\n",
       "  'the',\n",
       "  'story',\n",
       "  'never',\n",
       "  'really',\n",
       "  'gets',\n",
       "  'very',\n",
       "  'deep',\n",
       "  'and',\n",
       "  'there',\n",
       "  'are',\n",
       "  'a',\n",
       "  'lot',\n",
       "  'of',\n",
       "  'holes',\n",
       "  'in',\n",
       "  'the',\n",
       "  'plot',\n",
       "  'that',\n",
       "  'never',\n",
       "  'get',\n",
       "  'filled',\n",
       "  'this',\n",
       "  'just',\n",
       "  'wasn',\n",
       "  't',\n",
       "  'very',\n",
       "  'much',\n",
       "  'fun',\n",
       "  'despite',\n",
       "  'being',\n",
       "  'funny',\n",
       "  'at',\n",
       "  'times',\n",
       "  'br',\n",
       "  'br',\n",
       "  ''],\n",
       " ['although',\n",
       "  'coming',\n",
       "  'after',\n",
       "  'three',\n",
       "  'star',\n",
       "  'wars',\n",
       "  'krull',\n",
       "  'countless',\n",
       "  'others',\n",
       "  'this',\n",
       "  'movie',\n",
       "  'would',\n",
       "  'look',\n",
       "  'outdated',\n",
       "  'in',\n",
       "  'the',\n",
       "  's',\n",
       "  'sfx',\n",
       "  'mostly',\n",
       "  'consist',\n",
       "  'of',\n",
       "  's',\n",
       "  'videogames',\n",
       "  'effects',\n",
       "  'such',\n",
       "  'as',\n",
       "  'bolts',\n",
       "  'etc',\n",
       "  'annoying',\n",
       "  'after',\n",
       "  'a',\n",
       "  'short',\n",
       "  'while',\n",
       "  'you',\n",
       "  'also',\n",
       "  'get',\n",
       "  'a',\n",
       "  'sfx',\n",
       "  'creature',\n",
       "  'that',\n",
       "  'looks',\n",
       "  'like',\n",
       "  'a',\n",
       "  'poor',\n",
       "  'man',\n",
       "  's',\n",
       "  'version',\n",
       "  'of',\n",
       "  'some',\n",
       "  'tier',\n",
       "  'iv',\n",
       "  'harryhausen',\n",
       "  'monster',\n",
       "  'br',\n",
       "  'br',\n",
       "  'sets',\n",
       "  'are',\n",
       "  'mainly',\n",
       "  'ruins',\n",
       "  'in',\n",
       "  'the',\n",
       "  'countryside',\n",
       "  'with',\n",
       "  'papier',\n",
       "  'mache',\n",
       "  'temples',\n",
       "  'and',\n",
       "  'miniature',\n",
       "  'cities',\n",
       "  'or',\n",
       "  'abodes',\n",
       "  'that',\n",
       "  'makes',\n",
       "  's',\n",
       "  'japanese',\n",
       "  'monster',\n",
       "  'movies',\n",
       "  'look',\n",
       "  'like',\n",
       "  'flawless',\n",
       "  'perfection',\n",
       "  'br',\n",
       "  'br',\n",
       "  'plot',\n",
       "  'is',\n",
       "  'paper',\n",
       "  'extra',\n",
       "  'thin',\n",
       "  'hercules',\n",
       "  'must',\n",
       "  'find',\n",
       "  'zeus',\n",
       "  'seven',\n",
       "  'golden',\n",
       "  'thunderbolts',\n",
       "  'stolen',\n",
       "  'by',\n",
       "  'conspiring',\n",
       "  'gods',\n",
       "  'zombie',\n",
       "  'tyrants',\n",
       "  'br',\n",
       "  'br',\n",
       "  'action',\n",
       "  'mainly',\n",
       "  'consists',\n",
       "  'in',\n",
       "  'retarded',\n",
       "  'muscled',\n",
       "  'up',\n",
       "  'hercules',\n",
       "  'check',\n",
       "  'the',\n",
       "  'variety',\n",
       "  'of',\n",
       "  'facial',\n",
       "  'expressions',\n",
       "  'wrestling',\n",
       "  'cheap',\n",
       "  's',\n",
       "  'videogames',\n",
       "  'effects',\n",
       "  'br',\n",
       "  'br',\n",
       "  'acting',\n",
       "  'award',\n",
       "  'goes',\n",
       "  'to',\n",
       "  'milly',\n",
       "  'carlucci',\n",
       "  'third',\n",
       "  'carlucci',\n",
       "  'show',\n",
       "  'biz',\n",
       "  'sister',\n",
       "  'with',\n",
       "  'anna',\n",
       "  'gabriella',\n",
       "  'which',\n",
       "  'says',\n",
       "  'all',\n",
       "  'br',\n",
       "  'br',\n",
       "  'sfx',\n",
       "  'make',\n",
       "  'other',\n",
       "  'tier',\n",
       "  'ii',\n",
       "  'italian',\n",
       "  'salad',\n",
       "  'bowl',\n",
       "  'movies',\n",
       "  'such',\n",
       "  'as',\n",
       "  'l',\n",
       "  'umanoide',\n",
       "  'star',\n",
       "  'crash',\n",
       "  'look',\n",
       "  'like',\n",
       "  'masterpieces',\n",
       "  'br',\n",
       "  'br',\n",
       "  'well',\n",
       "  'considering',\n",
       "  'that',\n",
       "  'ferrigno',\n",
       "  's',\n",
       "  'main',\n",
       "  'acting',\n",
       "  'exploit',\n",
       "  'consisted',\n",
       "  'in',\n",
       "  'impersonating',\n",
       "  'a',\n",
       "  'retarded',\n",
       "  'green',\n",
       "  'monster',\n",
       "  'wearing',\n",
       "  'a',\n",
       "  'whig',\n",
       "  'and',\n",
       "  'green',\n",
       "  'espadrillas',\n",
       "  'we',\n",
       "  'ought',\n",
       "  'to',\n",
       "  'be',\n",
       "  'lenient',\n",
       "  'br',\n",
       "  'br',\n",
       "  'watch',\n",
       "  'it',\n",
       "  'forget',\n",
       "  'about',\n",
       "  'it',\n",
       "  '']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_neg_test[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To παραπάνω output είναι οι 2 πρώτες αρνητικές κριτικές του test set σε tokenized προτάσεις."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Κατασκευή Corpus στο οποίο ενώνουμε τα training data που είναι διαχωρισμένα σε negative , positive και φτιάχνουμε μια επιπλέον λίστα με τα αντίστοιχα labels 1 για positive  , 0 για negative.\n",
    "\n",
    "Στη συνέχεια επιστρέφουμε μια ανακατεμένη λίστα με 2 υπολίστες που περιέχουν παράλληλα και οι 2 τα δεδομένα reviews - labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = create_corpus(positive_train, negative_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Παρακάτω τυπώνουμε 2 κριτικές που είναι στην 1η υπολίστα του corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['she has been catapulted from to with magic dust involved courtesy the year old matt but nothing is made of that except as an unexplained device new york city especially central park but also every other slice of the place incorporated into the movie seems hope filled and easily livable and save for lucy there s no villain in jenna s adult life and even lucy is not cast as monstrous only as a nasty year old grown years more devious chris the one time boy object of jenna s yearning is now a porky cab driver and you have seen enough films to know that matt will play a major role in jenna s future you don t know quite what might impede this before it is finally achieved though i m here to whisper in your ear so to speak that the device is not unique in fact not only is this a variation on the theme of tom hanks big though nowhere near as fine it is also a strictly by the book version of this subset of the cinderella story ',\n",
       " 'this movie is a journey through the mind of a screenwriter caught in his own paradoxical philosophy he examines the ever illusive question of who am i and what is i it s a courageous and thought provoking enterprise there is a shipload of beautiful images dream inspired escher like paradoxes reminiscent of the hand drawing itself or rather erasing itself more and more we follow the writer in his agony over what to say and what to film we see him phoning with his wife who left for peru leaving him to take care of their baby a task he performs with less and less attention until he s so absorbed in his dilemma s that he hardly looks at the child anymore his wife comes back and makes a scene destroys his notes and helping him go over the last treshold until he erases him self interspersed with eye pleasing and i destructing images the story is mainly philosophical it s about the veils of maya the world of illusion the paradox of the movie however is that it needs a lot of talking and thinking to prove that thinking should stop during the more than two hours of provocative beauty and rapid philosophising the movie made me long for silence or a shorter movie if that was the purpose of the maker he succeeded quite well ']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Τώρα θα τυπώσουμε και τα 2 labels αυτών των κριτικών στη 2η υπολίστα της λίστας corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[1][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Όπως φαίνεται και οι 2 κριτικές είναι αρνητικές."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 17: Κατασκευή ΒΟW αναπαραστάσεων και ταξινόμηση"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(a)__\n",
    "\n",
    "Στην περίπτωση, που έχουμε bag of words matrix , σημαίνει ότι κάθε γραμμή  περιέχει το vocabulary μας και κάθε στήλη περιέχει ένα document. Δηλαδή έχουμε ένα πίνακα |v| x |d|. \n",
    "\n",
    "Ώστοσο, αν μείνουμε στο πόσες φορές απλά μια λέξη εμφανίζεται σε ένα document δεν βγάζουμε καλά συμπεράσματα εξαιτίας λέξεων που χρησιμοποιούντε συχνά όπως το the, it , they. Ουσιαστικά, μας ενδιαφέρουν λέξεις κοντά σε άλλες με μεγάλη συχνότητα αλλά όχι και με πολύ μεγάλη συχνότητα όπως οι λέξεις που αναφέραμε. \n",
    "\n",
    "Αυτό έρχεται να κάνει balance o αλγόριθμος tf-idf. Τα βάρη tf-idf είναι ένα γινόμενο 2 όρων, του term frequency και του inverse document frequency.\n",
    "\n",
    "O 1ος όρος δίνει βαρύτητα στις λέξεις που εμφανίζονται σε ένα κείμενο συχνά , ενώ ο 2ος όρος δίνει βαρύτητα σε λέξεις που είναι πιο πολύ σπάνιες, για παράδειγμα μπορεί να εμφανίζονται μόνο σε 1 κείμενο από τα 100 , ενώ άλλες λέξεις εμφανίζονται και στα 100 κείμενα. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(b)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "X = vectorizer.fit_transform(corpus[0])\n",
    "pickle.dump(vectorizer.vocabulary_,open(\"feature.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(c)__\n",
    "\n",
    "Aρχικά κάνουμε train το model με τα δεδομένα για εκπαίδευση που έχουμε."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(solver='lbfgs',max_iter=1000)\n",
    "y = corpus[1]\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Τώρα θα υπολογίσουμε το αccuracy στα test δεδομένα. Αρχικά φτίαχνω ένα corpus_test όπως πριν όμως για τα δεδομένα για πρόγνωση."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_test = create_corpus(positive_test, negative_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_vec = CountVectorizer(decode_error=\"replace\",vocabulary=pickle.load(open(\"feature.pkl\", \"rb\")))\n",
    "X_test = loaded_vec.fit_transform(corpus_test[0])\n",
    "y_test = corpus_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8579"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Όπως βλέπουμε έχουμε αρκετά υψηλό accuracy.\n",
    "\n",
    "Ο λόγος που χρησιμοποιήσαμε το module pickle ήταν για να αποθηκεύσουμε το εκπαιδευμένο μοντέλο σε ένα αρχείο ώστε μετά να κάνει transform το test set βάσει αυτού του μοντέλου και όχι άλλου, ώστε να έχουμε τα ίδια χαρακτηριστικά."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(d)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer(smooth_idf=True) #smooth_idf=True helps in division by 0\n",
    "tdidf_xtrain = transformer.fit_transform(X)\n",
    "tdidf_xtest  = transformer.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_tdf = LogisticRegression(solver='lbfgs',max_iter=1000)\n",
    "clf_tdf.fit(tdidf_xtrain,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8684"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_tdf.score(tdidf_xtest,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Παρατηρούμε πως υπάρχει βελτίωση στο SCORE με τη χρήση του αλγορίθμου td-idf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 18: Χρήση Word2Vec αναπαραστάσεων για ταξινόμηση."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Παρακάτω ακολουθεί κώδικας από το βήμα 9 της προπαρασκευαστικής για να πάρουμε τα κατάλληλα δεδομένα ξανά.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_sherlock = '/home/zerkes/Desktop/DSML - MASTER/1st semester/NLP/lab1/προπαρασκευαστικό/1book.txt' # path for the book!\n",
    "\n",
    "import re\n",
    "def tokenize(s):\n",
    "    s = s.strip().lower() # removing spaces in beginning and ending of a sentence + lowering the letters\n",
    "    s = re.sub(r'\\W+', ' ', s) # '\\W == [^a-zA-Z0-9_] keeping only alphanumerical values\n",
    "    s = s.replace('\\n', ' ')\n",
    "    s = s.split()\n",
    "    return s\n",
    "\n",
    "def corpus_sentence(path):\n",
    "    array=[]\n",
    "    file = open(path_sherlock)\n",
    "    lines = [line for line in file.readlines() if line.strip()] #keeping non empty lines\n",
    "    file.close()\n",
    "    for l in lines:\n",
    "        l = l.strip().lower()\n",
    "        l = re.sub(r'\\W+', ' ', l)\n",
    "        l = l.replace('\\n', ' ')\n",
    "        array.append(l)\n",
    "    return array   \n",
    "\n",
    "def corpus_sentence_tokenize(path):\n",
    "    array=[]\n",
    "    file = open(path_sherlock)\n",
    "    lines = [line for line in file.readlines() if line.strip()] #keeping non empty lines\n",
    "    file.close()\n",
    "    for l in lines:\n",
    "        array.append(tokenize(l))\n",
    "    return array   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_sentence_tkn = corpus_sentence_tokenize(path_sherlock) # tokenized sentences\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(sg=1,min_count=1,size=100,window=5)\n",
    "model.build_vocab(corpus_sentence_tkn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78007136, 109226000)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(corpus_sentence_tkn,total_examples=model.corpus_count, epochs=1000, compute_loss=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(a)__\n",
    "θα δούμε από το corpus του train πόσες λέξεις έχουμε OOV από το word2vec του ερωτήματος 9, ενώ ταυτόχρονα θα υπολογίσουμε και την κάθε πρόταση ως αναπαράσταση word2vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### oov ποσοστό"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.70524690516528\n"
     ]
    }
   ],
   "source": [
    "index2word_set = set(model.wv.index2word)\n",
    "oov_counter = 0\n",
    "n_words = 0\n",
    "for i in range(len(corpus[0])):\n",
    "    words = corpus[0][i].split()\n",
    "    for word in words:\n",
    "        if word not in index2word_set:\n",
    "            oov_counter +=1\n",
    "        n_words +=1\n",
    "print((oov_counter/n_words)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Παρατηρούμε ότι έχουμε ένα ποσοστό 18% λέξεω εκτός λεξιλογίου στο μοντέλο μας. Κάτι που είναι λογικό καθώς έχουμε πολύ μικρό dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yπολογισμός προτάσεων ως word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import spatial\n",
    "\n",
    "index2word_set = set(model.wv.index2word)\n",
    "\n",
    "def calculate_sentences(sentence, model,index2word_set,size=100):\n",
    "    words = sentence.split()\n",
    "    feature_vec = np.zeros((size, ), dtype='float32')\n",
    "    n_words = 0\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            n_words += 1\n",
    "            feature_vec = np.add(feature_vec, model[word])\n",
    "        else:\n",
    "            #word is OOV\n",
    "            feature_vec = np.add(feature_vec, np.zeros((size, ), dtype='float32'))\n",
    "    if (n_words > 0):\n",
    "        feature_vec = np.divide(feature_vec, n_words)\n",
    "    return feature_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2word_set = set(model.wv.index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zerkes/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "#corpus[i][0].split()\n",
    "sentences2vec_train=[]\n",
    "for i in range(len(corpus[0])):\n",
    "    sentences2vec_train.append(calculate_sentences(corpus[0][i], model, index2word_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zerkes/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "sentences2vec_test=[]\n",
    "for i in range(len(corpus_test[0])):\n",
    "    sentences2vec_test.append(calculate_sentences(corpus_test[0][i], model, index2word_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(β)__ Κάνουμε training με logistic regression και έπειτα βλέπουμε τα αποτελέσματα στο test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6958\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#logistic regression fit\n",
    "clf3 = LogisticRegression(solver='lbfgs',max_iter=1000)\n",
    "clf3.fit(sentences2vec_train,corpus[1])\n",
    "#predict\n",
    "print(clf3.score(sentences2vec_test,corpus_test[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Παρατηρούμε πως τα ποσοστά είναι αρκετά χαμηλά , γύρω στο 70%. Αυτό είναι λογικό καθώς στα δεδομένα μας, έχουμε αρκετές λέξεις που λείπουν από από τα δεδομένα που κάνουμε predict, ενώ επίσης έχουμε μικρό dataset για τον υπολογισμό word2vec διανυσμάτων, με αποτέλεσμα τα διανύσματα με κοντινές αποστάσεις, οι αντίστοιχες λέξεις τους να μην είναι κοντά σημασιολογικά."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(γ)__ , __(δ)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H μεταβλήτη NUM_W2V_TO_LOAD έχει μειωθεί στο 100 000 λόγω έλλειψης μνήμης."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model_google = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin',binary=True, limit=NUM_W2V_TO_LOAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#βάζουμε σε μια λίστα τις λέξεις που χρησιμοποιήσαμε στο ερώτημα 9\n",
    "random_words = ['vestige','allude','fulfill','jersey','restraint','property','assume','method','chuckled','fashioned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Similar words for:  vestige\n",
      "[('vestiges', 0.7638673186302185), ('remnant', 0.6451860666275024), ('relic', 0.6033318638801575), ('anachronism', 0.5288219451904297), ('bastion', 0.5030342936515808), ('bastions', 0.4654688835144043), ('bygone_era', 0.45959675312042236), ('remnants', 0.45440924167633057), ('anachronistic', 0.44333261251449585), ('throwback', 0.42959529161453247)]\n",
      "\n",
      "\n",
      "Most Similar words for:  allude\n",
      "[('alluded', 0.5936486124992371), ('alludes', 0.5497958660125732), ('imply', 0.5486440658569336), ('mention', 0.5430586338043213), ('depict', 0.5281932353973389), ('cite', 0.5100091099739075), ('foreshadow', 0.508596658706665), ('alluding', 0.5042288303375244), ('expound', 0.5022125840187073), ('describe', 0.49938517808914185)]\n",
      "\n",
      "\n",
      "Most Similar words for:  fulfill\n",
      "[('fulfilled', 0.7905531525611877), ('fulfilling', 0.784023642539978), ('fulfills', 0.7497856616973877), ('satisfy', 0.6402673721313477), ('meet', 0.5674575567245483), ('fulfillment', 0.5567388534545898), ('unfulfilled', 0.5346478223800659), ('uphold', 0.5098166465759277), ('accomplish', 0.4923442304134369), ('attain', 0.49219411611557007)]\n",
      "\n",
      "\n",
      "Most Similar words for:  jersey\n",
      "[('jerseys', 0.8257032036781311), ('shirt', 0.6591905355453491), ('shirts', 0.5496740341186523), ('sweater', 0.5234541893005371), ('uniform', 0.5117536783218384), ('T_shirt', 0.5113909840583801), ('sweatshirt', 0.5083307027816772), ('yellow_jersey', 0.5051953792572021), ('bib', 0.49717551469802856), ('t_shirt', 0.49341946840286255)]\n",
      "\n",
      "\n",
      "Most Similar words for:  restraint\n",
      "[('prudence', 0.5401768684387207), ('restraints', 0.5146862864494324), ('discipline', 0.5057604908943176), ('restrain', 0.49691110849380493), ('restrained', 0.4869222640991211), ('moderation', 0.4706111550331116), ('restraining', 0.4700942635536194), ('exercise_restraint', 0.4613053798675537), ('statesmanship', 0.4563831686973572), ('unrestrained', 0.44838494062423706)]\n",
      "\n",
      "\n",
      "Most Similar words for:  property\n",
      "[('properties', 0.6640733480453491), ('Property', 0.6595884561538696), ('land', 0.5981504917144775), ('real_estate', 0.5690802931785583), ('farmland', 0.5656534433364868), ('parcels', 0.5615769624710083), ('parcel', 0.5547882318496704), ('undeveloped_land', 0.5444345474243164), ('landowner', 0.5442891716957092), ('appraised_value', 0.544124960899353)]\n",
      "\n",
      "\n",
      "Most Similar words for:  assume\n",
      "[('assuming', 0.7463758587837219), ('assumed', 0.7382079362869263), ('assumes', 0.6577011346817017), ('presume', 0.613982081413269), ('assumption', 0.5500080585479736), ('believe', 0.5274755954742432), ('Assume', 0.5185110569000244), ('surmise', 0.48988109827041626), ('ascribe', 0.4792078137397766), ('infer', 0.47590383887290955)]\n",
      "\n",
      "\n",
      "Most Similar words for:  method\n",
      "[('methods', 0.774137020111084), ('technique', 0.6950438022613525), ('methodology', 0.6411086320877075), ('techniques', 0.6139282584190369), ('tactic', 0.5811327695846558), ('mechanism', 0.5512185096740723), ('Method', 0.5424207448959351), ('tool', 0.518945038318634), ('procedure', 0.5139045119285583), ('methodologies', 0.49582356214523315)]\n",
      "\n",
      "\n",
      "Most Similar words for:  chuckled\n",
      "[('laughed', 0.8110694885253906), ('chuckling', 0.7780483365058899), ('grinned', 0.7619146108627319), ('smiled', 0.7552381157875061), ('chuckle', 0.7084102630615234), ('joked', 0.6905171275138855), ('quipped', 0.6562340259552002), ('deadpanned', 0.6557281017303467), ('jokingly', 0.6521958112716675), ('joking', 0.6307655572891235)]\n",
      "\n",
      "\n",
      "Most Similar words for:  fashioned\n",
      "[('crafted', 0.6488865613937378), ('fashioning', 0.6323962807655334), ('carved', 0.5400882959365845), ('cobbled_together', 0.5067074298858643), ('molded', 0.49878501892089844), ('woven', 0.47563472390174866), ('shaped', 0.4614819884300232), ('pieced_together', 0.4606205224990845), ('constructed', 0.45962539315223694), ('transformed', 0.45502930879592896)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in random_words:\n",
    "    print(\"Most Similar words for: \",i)\n",
    "    print(model_google.most_similar(positive=i))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Παρατηρούμε πως με αυτό το πολύ μεγαλύτερο dataset , έχουμε πολύ καλύτερα αποτελέσματα και οι λέξεις είναι αρκετά κοντά νοηματικά."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(ε)__ Τώρα θα φτιάξουμε ένα logistic regression μοντέλο με neural bag of words αναπαραστάσεις , με βάση το word2vec της google."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zerkes/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "index_google = set(model_google.wv.index2word)\n",
    "sentences2google_train=[]\n",
    "for i in range(len(corpus[0])):\n",
    "    sentences2google_train.append(calculate_sentences(corpus[0][i], model_google, index_google,size=300))\n",
    "\n",
    "sentences2google_test=[]\n",
    "for i in range(len(corpus_test[0])):\n",
    "    sentences2google_test.append(calculate_sentences(corpus_test[0][i], model_google, index_google,size=300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8361\n"
     ]
    }
   ],
   "source": [
    "#logistic regression fit\n",
    "clf4 = LogisticRegression(solver='lbfgs',max_iter=1000)\n",
    "clf4.fit(sentences2google_train,corpus[1])\n",
    "#predict\n",
    "print(clf4.score(sentences2google_test,corpus_test[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Το ποσοστό όπως φαίνεται αυξήθηκε κατά 14%. Γεγονός, που σημαίνει πως όσο περισσότερα δεδομένα έχεις τόσο καλύτερα μπορείς να αναπαραστήσεις τις προτάσεις σου και να φτάσεις σε καλύτερες προγνώσεις."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(στ)__\n",
    "\n",
    "Τώρα θα δημιουργήσουμε αναπαραστάσεις όπως πριν αλλά με το συνδυασμό του td-idf. Αρχικά, το δοκιμάσουμε στο δικό μας μοντέλο και έπειτα στο επόμενω ερώτημα στης google.\n",
    "\n",
    "To td-idf θα το εκπαιδεύσουμε πάνω στο στο corpus των reviews του imdb στο training set, γιατί έχει πολλά documents.\n",
    "\n",
    "Στο scikit παίρνουμε τα score tdidf με τη χρήση του attribute _tfidf.idf_ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(use_idf=True)\n",
    "vectorizer.fit_transform(corpus[0])\n",
    "\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "idf = vectorizer._tfidf.idf_\n",
    "\n",
    "tdidf_dict = dict(zip(feature_names, idf)) # key: words , value: td-idf score\n",
    "\n",
    "def calculate_sentences_td_idf(sentence, model,index2word_set,size=100,tdidf_dict=tdidf_dict):\n",
    "    words = sentence.split()\n",
    "    feature_vec = np.zeros((size, ), dtype='float32')\n",
    "    n_words = 0\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            n_words += 1\n",
    "            if word in tdidf_dict.keys():\n",
    "                feature_vec = np.add(feature_vec , model[word] * tdidf_dict[word])\n",
    "            else:\n",
    "                feature_vec = np.add(feature_vec, model[word])\n",
    "        else:\n",
    "            #word is OOV\n",
    "            feature_vec = np.add(feature_vec, np.zeros((size, ), dtype='float32'))\n",
    "    if (n_words > 0):\n",
    "        feature_vec = np.divide(feature_vec, n_words)\n",
    "    return feature_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tώρα θα μετατρέψουμε με το word2vec το δικό μας τις προτάσεις σε NBOW ξανά αλλά με τη χρήση των td-idf score και θα δούμε αν έχει διαφορά στο αποτέλεσμα."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zerkes/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:21: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/home/zerkes/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:23: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6728\n"
     ]
    }
   ],
   "source": [
    "#corpus[i][0].split()\n",
    "sentences2vec_tdidf_train=[]\n",
    "for i in range(len(corpus[0])):\n",
    "    sentences2vec_tdidf_train.append(calculate_sentences_td_idf(corpus[0][i], model, index2word_set))\n",
    "\n",
    "sentences2vec_tdidf_test=[]\n",
    "for i in range(len(corpus_test[0])):\n",
    "    sentences2vec_tdidf_test.append(calculate_sentences_td_idf(corpus_test[0][i], model, index2word_set))\n",
    "\n",
    "#logistic regression fit\n",
    "clf4 = LogisticRegression(solver='lbfgs',max_iter=1000)\n",
    "clf4.fit(sentences2vec_tdidf_train,corpus[1])\n",
    "#predict\n",
    "print(clf4.score(sentences2vec_tdidf_test,corpus_test[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "παρατηρούμε ότι δεν υπήρχε σημαντική διαφορά στο αποτέλεσμα. Έχουμε μείωση του ποσοστού από 0.6958 που ήταν."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(ζ)__\n",
    "\n",
    "Σε αυτό το ερώτημα θα επαναλάβουμε τη διαδικασία του προηγούμενου ερωτήματος , αλλά στο word2vec της google."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zerkes/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8432\n"
     ]
    }
   ],
   "source": [
    "index_google = set(model_google.wv.index2word)\n",
    "sentences2google_train=[]\n",
    "for i in range(len(corpus[0])):\n",
    "    sentences2google_train.append(calculate_sentences_td_idf(corpus[0][i], model_google, index_google,size=300))\n",
    "\n",
    "sentences2google_test=[]\n",
    "for i in range(len(corpus_test[0])):\n",
    "    sentences2google_test.append(calculate_sentences_td_idf(corpus_test[0][i], model_google, index_google,size=300))\n",
    "\n",
    "#logistic regression fit\n",
    "clf5 = LogisticRegression(solver='lbfgs',max_iter=1000)\n",
    "clf5.fit(sentences2google_train,corpus[1])\n",
    "#predict\n",
    "print(clf5.score(sentences2google_test,corpus_test[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Στο συγκεκριμένο μοντέλο παρατηρούμε μια μικρή άυξηση του ποσοστού της τάξεως του 1%.\n",
    "Γεγονός, που σημαίνει ότι η χρήση των βαρών td-idf ταιριάζει καλύτερα στο word2vec της google, ίσως γιατί έχει δημιουργήσει το μοντέλο της πάνω σε ένα τεράστιο όγκο δεδομένων."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 19 : Extra Credit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(a)__ Αρχικά θα πειραματιστούμε με άλλους ταξινομητές όπως KNN , SVM για να δούμε τι ποσοστά θα μας δώσουνε."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__KNN__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mε το word2vec της google:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7529\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=5)\n",
    "neigh.fit(sentences2google_train, corpus[1])\n",
    "print(neigh.score(sentences2google_test,corpus_test[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mε το δικό μας word2vec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6215\n"
     ]
    }
   ],
   "source": [
    "neigh2 = KNeighborsClassifier(n_neighbors=5)\n",
    "neigh2.fit(sentences2vec_train, corpus[1])\n",
    "print(neigh2.score(sentences2vec_test,corpus_test[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__SVM__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oμοίως με τη χρήση αλγορίθμου SVM για το μοντέλο της google. Κάνουμε χρήση του kernel rbf καθώς έχουμε κλάσεις που δεν είναι γραμμικά διαχωρίσιμες."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zerkes/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8069\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svmclass = SVC(kernel='rbf')\n",
    "svmclass.fit(sentences2google_train, corpus[1])\n",
    "print(svmclass.score(sentences2google_test,corpus_test[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Για το δικό μας μοντέλο"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zerkes/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.655\n"
     ]
    }
   ],
   "source": [
    "svmclass2 = SVC(kernel='rbf')\n",
    "svmclass2.fit(sentences2vec_train, corpus[1])\n",
    "print(svmclass2.score(sentences2vec_test,corpus_test[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Από ότι φαίνεται τα μοντέλα SVM , KNN είναι χειρότερες επιλογές από το logistic regression. Επίσης, είναι εμφανές ότι πάνω στο δικό μας word2vec είναι ακόμα χειρότερα τα ποσοστά, καθώς έχουμε πολύ μικρότερο όγκο δεδομένων από τη google."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(b)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__fastText__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fasttext\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/61/2e01f1397ec533756c1d893c22d9d5ed3fce3a6e4af1976e0d86bb13ea97/fasttext-0.9.1.tar.gz (57kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 519kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pybind11>=2.2 in /home/zerkes/anaconda3/lib/python3.7/site-packages (from fasttext) (2.4.3)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /home/zerkes/anaconda3/lib/python3.7/site-packages (from fasttext) (41.0.1)\n",
      "Requirement already satisfied: numpy in /home/zerkes/anaconda3/lib/python3.7/site-packages (from fasttext) (1.16.4)\n",
      "Building wheels for collected packages: fasttext\n",
      "  Building wheel for fasttext (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/zerkes/.cache/pip/wheels/9f/f0/04/caa82c912aee89ce76358ff954f3f0729b7577c8ff23a292e3\n",
      "Successfully built fasttext\n",
      "Installing collected packages: fasttext\n",
      "Successfully installed fasttext-0.9.1\n"
     ]
    }
   ],
   "source": [
    "!pip install fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Διαφορές fasttext με word2vec\n",
    "\n",
    "Το μοντέλο της google (word2vec) αντιμετωπίζει την κάθε λέξη σαν μια οντότητα. Σε αντίθεση το fasttext αντιμετωπίζει την κάθε λέξη σαν σύνθεση χαρακτήρων n-grams. Για παράδειγμα για τη λέξη \"text\" έχουμε το άθροισμα των διανυσμάτων των n-grams π.χ. \"te\" ,\"tex\" , \"text\" ,\"xt\", \"ext\"...\n",
    "\n",
    "Αυτό έχει ως αποτέλεσμα να έχουμε καλύτερα word embeddings για σπάνιες λέξεις, καθώς στο word2vec μια σπάνια λέξη έχει λίγους γείτονες ώστοσο τα n-grams της μοιράζοντε με αρκετές άλλες λέξεις.\n",
    "\n",
    "Ωστόσο το fasttext είναι πολύ πιο αργό στο training από το word2vec και ειδικά για μεγάλα dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "θα κάνω train το fasttext στο corpus τoυ βιβλίου που κατεβάσαμε. Και έπειτα θα δω πόσο ακριβή similarities έχω στις 10 λέξεις που τσεκάραμε και πόσο καλό accuracy έχει στο prediction των imdb reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_model = model = FastText(size=20, window=5, min_count=1) \n",
    "corpus_sentence_tkn = corpus_sentence_tokenize(path_sherlock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_model.build_vocab(sentences=corpus_sentence_tkn)\n",
    "fast_model.train(sentences=corpus_sentence_tkn, total_examples=fast_model.corpus_count, epochs=1000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Χρησιμοποιούμε size=20 λόγω έλλειψης μνήμης."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tώρα θα δούμε τα similarities για τις 10 λέξεις που είχαμε δει προηγουμένως με το δικό μας μοντέλο στο βιβλίο που διαλέξαμε."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Similar words for:  vestige\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zerkes/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('pronounce', 0.8173289895057678), ('revellers', 0.7642024755477905), ('form', 0.7633427381515503), ('irresistible', 0.7490476965904236), ('detained', 0.7369130849838257), ('descent', 0.7355394959449768), ('attained', 0.7329529523849487), ('representative', 0.7312352657318115), ('valet', 0.7256011962890625), ('force', 0.7237144708633423)]\n",
      "\n",
      "\n",
      "Most Similar words for:  allude\n",
      "[('alive', 0.7770902514457703), ('forget', 0.6869080066680908), ('lose', 0.6856810450553894), ('stay', 0.6743953824043274), ('neither', 0.6619927883148193), ('exclude', 0.6454653739929199), ('stiff', 0.6434610486030579), ('philadelphia', 0.6414873600006104), ('do', 0.6405911445617676), ('wit', 0.6259635090827942)]\n",
      "\n",
      "\n",
      "Most Similar words for:  fulfill\n",
      "[('fulfil', 0.90674889087677), ('wreck', 0.7742279767990112), ('college', 0.7578479647636414), ('michael', 0.7531304359436035), ('foppishness', 0.7388917207717896), ('shepherd', 0.7387361526489258), ('swamp', 0.7336680889129639), ('chat', 0.7333285212516785), ('cotton', 0.7332035303115845), ('pince', 0.725027322769165)]\n",
      "\n",
      "\n",
      "Most Similar words for:  jersey\n",
      "[('64', 0.725212574005127), ('victoria', 0.7235170602798462), ('hotels', 0.721388578414917), ('6221541', 0.720341682434082), ('horsey', 0.7186737656593323), ('1846', 0.7155656218528748), ('prima', 0.7004246711730957), ('diggings', 0.6976081728935242), ('victory', 0.6955941915512085), ('1858', 0.6931614875793457)]\n",
      "\n",
      "\n",
      "Most Similar words for:  restraint\n",
      "[('restrain', 0.8721491098403931), ('flatter', 0.8315050601959229), ('boswell', 0.7880375981330872), ('bearing', 0.7862269282341003), ('braving', 0.7838948369026184), ('expressions', 0.7793245911598206), ('prominently', 0.775829017162323), ('striking', 0.7750718593597412), ('inimitably', 0.7734323740005493), ('remonstrance', 0.7728161811828613)]\n",
      "\n",
      "\n",
      "Most Similar words for:  property\n",
      "[('proper', 0.8866829872131348), ('hoard', 0.821624219417572), ('hesitating', 0.7810395956039429), ('hesitation', 0.7775986194610596), ('provision', 0.7768787145614624), ('concealment', 0.7698445320129395), ('confession', 0.765796422958374), ('estate', 0.7493798136711121), ('provisions', 0.7481154799461365), ('prosperity', 0.7472187876701355)]\n",
      "\n",
      "\n",
      "Most Similar words for:  assume\n",
      "[('entirely', 0.8301395177841187), ('errand', 0.7597740888595581), ('supplementing', 0.7585780620574951), ('imprudence', 0.7496882081031799), ('august', 0.7260339260101318), ('augustine', 0.7209899425506592), ('notorious', 0.7190213203430176), ('equal', 0.7120595574378967), ('axiom', 0.7096044421195984), ('duty', 0.6981685161590576)]\n",
      "\n",
      "\n",
      "Most Similar words for:  method\n",
      "[('methods', 0.7916417121887207), ('promises', 0.7444714903831482), ('player', 0.7373656034469604), ('negligence', 0.7350733280181885), ('treasure', 0.7346911430358887), ('literature', 0.7315567135810852), ('manifestations', 0.7283042669296265), ('occupations', 0.7228367924690247), ('debts', 0.7198765277862549), ('nonsense', 0.7197486162185669)]\n",
      "\n",
      "\n",
      "Most Similar words for:  chuckled\n",
      "[('rearranging', 0.7793387174606323), ('chucked', 0.7770880460739136), ('took', 0.7674569487571716), ('shutting', 0.7651987075805664), ('lowered', 0.7421113848686218), ('raising', 0.7409968376159668), ('composed', 0.7321804761886597), ('drawled', 0.7320862412452698), ('meeting', 0.7307021021842957), ('pulled', 0.7265530824661255)]\n",
      "\n",
      "\n",
      "Most Similar words for:  fashioned\n",
      "[('stocked', 0.8559253215789795), ('blooded', 0.841632604598999), ('mottled', 0.8393630385398865), ('piled', 0.8127068877220154), ('sat', 0.8116410374641418), ('motioned', 0.8065500259399414), ('crouched', 0.800539493560791), ('oppressed', 0.8002431392669678), ('outstretched', 0.7988911271095276), ('buried', 0.7911409735679626)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in random_words:\n",
    "    print(\"Most Similar words for: \",i)\n",
    "    print(fast_model.most_similar(positive=i))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Σε αυτό το κομμάτι θα κάνουμε ένα training-test με logistic regression όπως πριν αλλά με το fasttext model.\n",
    "\n",
    "H σύγκριση αυτή είναι για να δούμε ποιο από τα 2 μοντέλα μας προσφέρει καλύτερο accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zerkes/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.642\n"
     ]
    }
   ],
   "source": [
    "index_fast = set(fast_model.wv.index2word)\n",
    "sentences2fast_train=[]\n",
    "for i in range(len(corpus[0])):\n",
    "    sentences2fast_train.append(calculate_sentences(corpus[0][i], fast_model, index_fast,size=20))\n",
    "\n",
    "sentences2fast_test=[]\n",
    "for i in range(len(corpus_test[0])):\n",
    "    sentences2fast_test.append(calculate_sentences(corpus_test[0][i], fast_model, index_fast,size=20))\n",
    "\n",
    "#logistic regression fit\n",
    "clffast = LogisticRegression(solver='lbfgs',max_iter=1000)\n",
    "clffast.fit(sentences2fast_train,corpus[1])\n",
    "#predict\n",
    "print(clffast.score(sentences2fast_test,corpus_test[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Παρατηρούμε πως έχουμε ένα ποσοστό 64% , που είναι 5% λιγότερο από του fast model. Αυτό μπορεί να συμβαίνει διότι στο fasttext έχουμε χρησιμοποιήσει size = 20 και όχι 100 όπως στο word2vec λόγω έλλειψης μνήμης.\n",
    "\n",
    "Μπορούμε να φανταστούμε πως με ένα αρκετά καλύτερο dataset και μεγαλύτερη μνήμη για να το κάνουμε training σε μεγαλύτερο size θα έχουμε αρκετά καλύτερα αποτελέσματα σε σχέση με το word2vec μοντέλο."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
